<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Robotics | Mingxi Jia</title><link>https://saulbatman.github.io/tag/robotics/</link><atom:link href="https://saulbatman.github.io/tag/robotics/index.xml" rel="self" type="application/rss+xml"/><description>Robotics</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 18 Aug 2022 00:00:00 +0000</lastBuildDate><image><url>https://saulbatman.github.io/media/icon_hu4c220a5ca0b827d3eca32f82a9dace98_1895126_512x512_fill_lanczos_center_3.png</url><title>Robotics</title><link>https://saulbatman.github.io/tag/robotics/</link></image><item><title>SEIL: Simulation-augmented Equivariant Imitation Learning</title><link>https://saulbatman.github.io/project/seil/</link><pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate><guid>https://saulbatman.github.io/project/seil/</guid><description>&lt;h1 id="authors">Authors&lt;/h1>
&lt;p>Mingxi Jia, Dian Wang, Guanang Su, David Klee, Xupeng Zhu, Robin Walters, Robert Platt&lt;/p>
&lt;h1 id="abstract">Abstract&lt;/h1>
&lt;p>In robotic manipulation, acquiring samples is extremely expensive because it often requires interacting with the real world. Traditional image-level data augmentation has shown the potential to improve sample efficiency in various machine learning tasks. However, image-level data augmentation is insufficient for an imitation learning agent to learn good manipulation policies in a reasonable amount of demonstrations. We propose Simulation-augmented Equivariant Imitation Learning (SEIL), a method that combines a novel data augmentation strategy of supplementing expert trajectories with simulated transitions and an equivariant model that exploits the $\mathbf{O}(2)$ symmetry in robotic manipulation. Experimental evaluations demonstrate that our method can learn non-trivial manipulation tasks within ten demonstrations and outperforms the baselines with a significant margin.&lt;/p>
&lt;h1 id="approach">Approach&lt;/h1>
&lt;h2 id="1-equivariant-behavioral-cloning">1. Equivariant Behavioral Cloning&lt;/h2>
&lt;img src="./img/equivariant.gif" alt="drawing" width="900"/>
&lt;h2 id="2-transition-simulation">2. Transition Simulation&lt;/h2>
&lt;img src="./img/TS.gif" alt="drawing" width="900"/>
&lt;h1 id="results">Results&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>Simulation results&lt;/p>
&lt;!-- ![Sim Results](./img/sim_result.png) -->
&lt;img src="./img/sim_result.png" alt="drawing" width="900"/>
&lt;/li>
&lt;li>
&lt;p>Real-world results&lt;/p>
&lt;img src="./img/real_result.png" alt="drawing" width="1000"/>
&lt;/li>
&lt;/ul>
&lt;h1 id="experiments">Experiments&lt;/h1>
&lt;h2 id="real-world-tasks">Real-world tasks&lt;/h2>
&lt;ul>
&lt;li>Block in Bowl
&lt;img src="./img/bowl.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Trash Tidying
&lt;img src="./img/trash3.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Shoe Packing
&lt;img src="./img/shoe.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Drawer Opening
&lt;img src="./img/drawer.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;/ul>
&lt;h2 id="interactive-close-loop-bahevior">Interactive close-loop bahevior&lt;/h2>
&lt;ul>
&lt;li>Interactive Drawers
&lt;img src="./img/Drawer_interactive.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Interactive &amp;ldquo;Trash Tidying&amp;rdquo;
&lt;img src="./img/Trash3_interactive.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;/ul>
&lt;h1 id="citation">Citation&lt;/h1>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">@article{jia2022seil,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> title={SEIL: Simulation-augmented Equivariant Imitation Learning},
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> author={Jia, Mingxi; Wang, Dian; Su, Guanang; Klee, David; Zhu, Xupeng; Walters, Robin; Platt, Robert},
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> journal={arXiv preprint arXiv:2211.00194},
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> year={2022}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Hindsight Experience Replay in Robotics Manipulation</title><link>https://saulbatman.github.io/project/her/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://saulbatman.github.io/project/her/</guid><description>&lt;p>In goal-conditioned reinforcement learning
problems, the sample efficiency is often a drawback that
most of the explorations are not consider as very useful
experience because they are failure episodes, which makes
low sample efficiency. In this paper, we implemented a
module of Hindsight Experience Replay (HER) in several
goal-conditioned environments, to discover its utility of
improving sample efficiency. Based on Deep Deterministic
Policy Gradient (DDPG), the experiments showed that the
HER module helps the agent learn much faster with more
robustness. We then discussed about the limitation of HER
and how hyper parameters effects its performance.&lt;/p></description></item><item><title>ORBSLAM3 on iPhone</title><link>https://saulbatman.github.io/project/orbslam/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://saulbatman.github.io/project/orbslam/</guid><description>&lt;p>In order to run our method, you should install ORBSLAM3 and Droidcam on your UBUNTU PC. And, you should install droidcam in your iPhone, which you can find in App store.
ORBSLAM3 and droidcam repositories have already been forked into our github repo. Please refer to the installation links mentioned above to download these necessary components. It is highly recommended to check the official github repo. We also get a modified version monoVO-Python here as our baseline, where we also add out kitti toolkits.
Our project is ORBSLAM3 on iPhone. The motivation is that SLAM often requires cameras like realsense, which is hard to get in daily life. So, why not use the camera in our iPhone? By using iPhone, many interesting applications can be furtherly extended, such as augmented reality, indoor localization, etc.
For now, the ORBSLAM algorithm runs on PC and the video sequence is transmitting from iPhone to PC via droidcam.&lt;/p>
&lt;p>Please refer to our &lt;a href="https://gitlab.com/saulbatman/eece5554/-/tree/main/FinalProject" target="_blank" rel="noopener">project website&lt;/a>! We also have a &lt;a href="https://github.com/SaulBatman/monoVO-python" target="_blank" rel="noopener">toolbox&lt;/a>&lt;/p></description></item></channel></rss>