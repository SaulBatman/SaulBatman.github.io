<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning | Mingxi Jia</title><link>https://saulbatman.github.io/tag/deep-learning/</link><atom:link href="https://saulbatman.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml"/><description>Deep Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 27 Aug 2022 00:00:00 +0000</lastBuildDate><image><url>https://saulbatman.github.io/media/icon_hu4c220a5ca0b827d3eca32f82a9dace98_1895126_512x512_fill_lanczos_center_3.png</url><title>Deep Learning</title><link>https://saulbatman.github.io/tag/deep-learning/</link></image><item><title>SEIL: Simulation-augmented Equivariant Imitation Learning</title><link>https://saulbatman.github.io/project/seil/</link><pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate><guid>https://saulbatman.github.io/project/seil/</guid><description>&lt;h1 id="abstract">Abstract&lt;/h1>
&lt;p>In robotic manipulation, acquiring samples is extremely expensive because it often requires interacting with the real world. Traditional image-level data augmentation has shown the potential to improve sample efficiency in various machine learning tasks. However, image-level data augmentation is insufficient for an imitation learning agent to learn good manipulation policies in a reasonable amount of demonstrations. We propose Simulation-augmented Equivariant Imitation Learning (SEIL), a method that combines a novel data augmentation strategy of supplementing expert trajectories with simulated transitions and an equivariant model that exploits the $\mathbf{O}(2)$ symmetry in robotic manipulation. Experimental evaluations demonstrate that our method can learn non-trivial manipulation tasks within ten demonstrations and outperforms the baselines with a significant margin.&lt;/p>
&lt;h1 id="approach">Approach&lt;/h1>
&lt;h2 id="1-equivariant-behavioral-cloning">1. Equivariant Behavioral Cloning&lt;/h2>
&lt;img src="./img/equivariant.gif" alt="drawing" width="900"/>
&lt;h2 id="2-transition-simulation">2. Transition Simulation&lt;/h2>
&lt;img src="./img/TS.gif" alt="drawing" width="900"/>
&lt;h1 id="results">Results&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>Simulation results&lt;/p>
&lt;!-- ![Sim Results](./img/sim_result.png) -->
&lt;img src="./img/sim_result.png" alt="drawing" width="900"/>
&lt;/li>
&lt;li>
&lt;p>Real-world results&lt;/p>
&lt;img src="./img/real_result.png" alt="drawing" width="1000"/>
&lt;/li>
&lt;/ul>
&lt;h1 id="experiments">Experiments&lt;/h1>
&lt;h2 id="real-world-tasks">Real-world tasks&lt;/h2>
&lt;ul>
&lt;li>Block in Bowl
&lt;img src="./img/bowl.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Trash Tidying
&lt;img src="./img/trash3.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Shoe Packing
&lt;img src="./img/shoe.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Drawer Opening
&lt;img src="./img/drawer.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;/ul>
&lt;h2 id="interactive-close-loop-bahevior">Interactive close-loop bahevior&lt;/h2>
&lt;ul>
&lt;li>Interactive Drawers
&lt;img src="./img/Drawer_interactive.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;li>Interactive &amp;ldquo;Trash Tidying&amp;rdquo;
&lt;img src="./img/Trash3_interactive.gif" alt="drawing" width="1000"/>&lt;/li>
&lt;/ul></description></item><item><title>Hindsight Experience Replay in Robotics Manipulation</title><link>https://saulbatman.github.io/project/her/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://saulbatman.github.io/project/her/</guid><description>&lt;p>In goal-conditioned reinforcement learning
problems, the sample efficiency is often a drawback that
most of the explorations are not consider as very useful
experience because they are failure episodes, which makes
low sample efficiency. In this paper, we implemented a
module of Hindsight Experience Replay (HER) in several
goal-conditioned environments, to discover its utility of
improving sample efficiency. Based on Deep Deterministic
Policy Gradient (DDPG), the experiments showed that the
HER module helps the agent learn much faster with more
robustness. We then discussed about the limitation of HER
and how hyper parameters effects its performance.&lt;/p></description></item><item><title>Silhouette-based Object 3D reconstruction</title><link>https://saulbatman.github.io/project/turbinenet/</link><pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate><guid>https://saulbatman.github.io/project/turbinenet/</guid><description>&lt;h1 id="mesh-simplification">Mesh Simplification&lt;/h1>
&lt;!-- &lt;img src="./img/sim.jpg" alt="drawing" width="1000"/> -->
&lt;p>3D object meshes often contains millions of redundant vertices and faces. This tool is based on &lt;a href="http://mgarland.org/files/papers/quadrics.pdf" target="_blank" rel="noopener">&amp;ldquo;Surface Simplification Using Quadric Error Metrics&amp;rdquo;&lt;/a> , and it can help you to reduce points in your mesh model. Codes can be found &lt;a href="https://github.com/SaulBatman/Mesh_Simplify" target="_blank" rel="noopener">here&lt;/a>&lt;/p>
&lt;h1 id="maya-3d-mesh-modeling">MAYA 3D mesh modeling&lt;/h1>
&lt;p>The toolbos is based on Autodesk MAYA and Python, including tools such as skeleton manipulation, automatic disturbance, and batch outputting. Please refer to &lt;a href="https://github.com/SaulBatman/PCA-MAYA-tools" target="_blank" rel="noopener">this repository&lt;/a>.&lt;/p>
&lt;h1 id="turbinenet">TurbineNet&lt;/h1>
&lt;ul>
&lt;li>Coming soon!&lt;/li>
&lt;/ul></description></item></channel></rss>